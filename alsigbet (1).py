# -*- coding: utf-8 -*-
"""ALSIGBET.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11PSgwjZeRlFKRIFcj7e4-voVnn6xoa8U
"""

X_train = ["This was really awesome an awesome movie",
           "Great movie! Ilikes it a lot",
           "Happy Ending! Awesome Acting by hero",
           "loved it!",
           "Bad not upto the mark",
           "Could have been better",
           "really Dissapointed by the movie"]
# X_test = "it was really awesome and really disspntd"

y_train =["1","1","1","0","0","0","0"] # 1- Positive class, 0- negative class

plt.hist(y_train, bins=20)
plt.xlabel('Feature Values')
plt.ylabel('Frequency')
plt.title('Feature Distribution')
plt.show()

plt.hist(X_train, bins=40)
plt.xlabel('Feature Values')
plt.ylabel('Frequency')
plt.title('Feature Distribution')
plt.show()

plt.scatter(X_train, y_train,c='red')
plt.xlabel('Review ')
plt.ylabel('class')
plt.title('Scatter Plot')
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix,accuracy_score, classification_report
from sklearn.linear_model import LinearRegression

X_train

from nltk.tokenize import RegexpTokenizer
import seaborn as sns

from nltk.stem.porter import PorterStemmer
# NLTK -> Stem -> Porter -> PorterStemmer

from nltk.corpus import stopwords

import nltk
nltk.download('stopwords')

tokenizer = RegexpTokenizer(r"\w+")
en_stopwords = set(stopwords.words('english'))
ps = PorterStemmer()

def getCleanedText(text):
  text = text.lower()

  # tokenizing
  tokens = tokenizer.tokenize(text)
  new_tokens = [token for token in tokens if token not in en_stopwords]
  stemmed_tokens = [ps.stem(tokens) for tokens in new_tokens]
  clean_text = " ".join(stemmed_tokens)
  return clean_text

X_test = ["it was bad"]

X_clean = [getCleanedText(i) for i in X_train]
xt_clean = [getCleanedText(i) for i in X_test]

X_clean

X_train = ["This was awesome an awesome movie",
           "Great movie! Ilikes it a lot",
           "Happy Ending! Awesome Acting by hero",
           "loved it!",
           "Bad not upto the mark",
           "Could have been better",
           "Dissapointed by the movie"]

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

cv = CountVectorizer(ngram_range = (1,2))

X_vec = cv.fit_transform(X_clean).toarray()

feature_names = cv.get_feature_names_out()

X_vec

print(cv.get_feature_names_out())

Xt_vect = cv.transform(xt_clean).toarray()

Xt_vect

from sklearn.naive_bayes import MultinomialNB

mn = MultinomialNB()

mn.fit(X_vec, y_train)

y_pred = mn.predict(Xt_vect)

y_pred

X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2,random_state=42)

from sklearn.svm import SVC

model = LogisticRegression()

vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

model.fit(X_train_tfidf, y_train)

y_pred = model.predict(X_test_tfidf)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy*100:.2f}")